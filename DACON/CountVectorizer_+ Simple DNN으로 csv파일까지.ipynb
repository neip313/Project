{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SIMPLE_DNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M229VB_Df_8u"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYxrBJDngC2D"
      },
      "source": [
        "## * sentencepiece는 직접 설치가 필요합니다 *\n",
        "### py파일을 실행하기 전에 pip install sentencepiece를 입력해 주세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBlyatR0YAyJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "f8dc4b8f-3134-4916-b77f-5e5bd9aa6425"
      },
      "source": [
        "from pandas import read_csv\n",
        "import re\n",
        "from sentencepiece import SentencePieceTrainer,SentencePieceProcessor\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import  EarlyStopping\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from keras.models import load_model\n",
        "from numpy import mean\n",
        "from pandas import DataFrame\n",
        "from random import randint\n",
        "\n",
        "# 코드에 필요한 파일 : data, models, submissions\n",
        "# import os\n",
        "# os.mkdir('/content/drive/MyDrive/Dacon-소설작가분류/models')\n",
        "# os.mkdir('/content/drive/MyDrive/Dacon-소설작가분류/submissions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0a28cb935e9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentencepiece\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentencepiece'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBuudrvRgjvC"
      },
      "source": [
        "# Data Loading\n",
        "## 1. file에서 data 불러오기\n",
        "## 2. Sentence Piece로 모델 학습\n",
        "## 3. CountVectorizer에 tokenizer로 trained model 사용\n",
        "## 4. 학습용 데이터 최종 Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUN4sHLIf9Sz"
      },
      "source": [
        "def data_loading():\n",
        "  \n",
        "  # 저는 data라는 폴더 안에 데이터를 저장해 두었습니다.\n",
        "\n",
        "  train = read_csv('/content/drive/MyDrive/Dacon-소설작가분류/data/train.csv', engine='python')\n",
        "  test = read_csv('/content/drive/MyDrive/Dacon-소설작가분류/data/test_x.csv', engine='python')\n",
        "  y_train = train['author'].values\n",
        "  print('data load completed')\n",
        "\n",
        "\n",
        "  # 필요없는 단어 제거\n",
        "  def alpha_num(text):\n",
        "      return re.sub(r'[^a-zA-z0-9\\s]', '', text)\n",
        "\n",
        "  # 소문자로 변환\n",
        "  train['text'] = train['text'].str.lower().apply(alpha_num)\n",
        "  test['text'] = test['text'].str.lower().apply(alpha_num)\n",
        "  print('data transformation completed')\n",
        "  del alpha_num\n",
        "\n",
        "  # SentencePieceTrainer로 학습할 데이터 준비\n",
        "  with open('author.txt','w',encoding='utf-8') as f:\n",
        "      f.write('\\n'.join(train['text']))\n",
        "  del f\n",
        "\n",
        "  # 3000자까지 vocab size를 지정해서 학습\n",
        "  SentencePieceTrainer.Train('--input=author.txt --model_prefix=author --vocab_size=3000')\n",
        "  \n",
        "  sp = SentencePieceProcessor()\n",
        "  sp.Load(\"author.model\")\n",
        "\n",
        "  # 학습된 모델을 tokenizer로 사용 / min_df = 3은 최소 3번 이상 나타난 단어를 의미합니다\n",
        "  cv = CountVectorizer(lowercase = False, tokenizer = sp.encode_as_pieces, min_df = 3)\n",
        "\n",
        "  # 최종적으로 사용할 학습데이터입니다.\n",
        "  tdm_train = cv.fit_transform(train['text']).toarray()\n",
        "  del train\n",
        "  tdm_test = cv.transform(test['text']).toarray()\n",
        "  print('data setting completed')\n",
        "\n",
        "  return tdm_train, tdm_test, y_train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bglitiVihGu2"
      },
      "source": [
        "# Simple DNN Code\n",
        "## Input Shape는 vocabsize = 3000을 기준으로 약 3000개의 단어가 들어갑니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQJDeb_ogivV"
      },
      "source": [
        "# 간단한 DNN 모델\n",
        "def create_dnn(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, activation='relu', input_shape = (input_shape,)))\n",
        "    model.add(Dropout(0.15))\n",
        "    model.add(Dense(256, activation = 'relu'))\n",
        "    model.add(Dropout(0.15))\n",
        "    model.add(Dense(64, activation = 'relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(16, activation = 'relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = Adam())\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qkYm67mhQdh"
      },
      "source": [
        "# Creating CSV\n",
        "## 1. StratifiedKFold로 5 fold\n",
        "## 2. val_loss를 기준으로 Early Stopping &  Model Checkpoint monitoring\n",
        "## 3. Checked Model 불러와 tdm_test 예측 + predicts에 저장\n",
        "## 4. log_loss로 Validation data score 저장\n",
        "## 5. 5 Fold의 predicts의 mean => csv로 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpnd6mGqhPeL"
      },
      "source": [
        "# csv를 만드는 코드\n",
        "def creating_results(seed, tdm_train, tdm_test, y_train):\n",
        "  batch_size =  256\n",
        "  epochs = 30\n",
        "\n",
        "  # SKF 중 생성되는 결과물 저장 \n",
        "  predicts = []\n",
        "  # Validation Score 저장\n",
        "  scores = []\n",
        "  \n",
        "  \n",
        "  # 과적합이 쉽게 되어 Patience는 2\n",
        "  es = EarlyStopping(monitor='val_loss', verbose=0, patience=2)\n",
        "\n",
        "  # 학습하며 생되는 model 저장\n",
        "  filepath_val_loss=\"/content/drive/MyDrive/Dacon-소설작가분류/models/best_model_cdnn.tf\"\n",
        "  checkpoint_val_loss = ModelCheckpoint(filepath_val_loss, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "\n",
        "  # SKF\n",
        "  skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "  for train_index, val_index in skf.split(tdm_train, y_train):\n",
        "    model = create_dnn(tdm_train.shape[1])\n",
        "    tr_X = tdm_train[train_index]\n",
        "    tr_y = y_train[train_index]\n",
        "\n",
        "    val_X = tdm_train[val_index]\n",
        "    val_y = y_train[val_index]\n",
        "\n",
        "    model.fit(tr_X,tr_y,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs, \n",
        "                        validation_data=(val_X,val_y), \n",
        "                        shuffle=True, \n",
        "                        verbose=0,\n",
        "                        callbacks=[es,checkpoint_val_loss]\n",
        "                        ) \n",
        "    \n",
        "    model = load_model(filepath_val_loss)\n",
        "    predicts.append(model.predict_proba(tdm_test))\n",
        "    score = log_loss(val_y,model.predict_proba(val_X))\n",
        "    scores.append(score)\n",
        "    del tr_X,tr_y,val_X,val_y,model,score\n",
        "  \n",
        "  # predicts에 저장된 predicts 저장\n",
        "  prediction = DataFrame(mean(predicts,axis=0)).reset_index()\n",
        "\n",
        "  # 대략적으로 어느정도의 validation loss를 가지고있는지 파악하기 위해 점수도 함께 저장\n",
        "  score = mean(scores)\n",
        "  print(score)\n",
        "  file_path = '/content/drive/MyDrive/Dacon-소설작가분류/submissions/SIMPLE_DNN_'+str(score)[:8]+'.csv'\n",
        "  prediction.to_csv(file_path, index=False) \n",
        "  print('#'*25,\"FINISHED  : \",score, ' '*10,'#'*25)\n",
        "  del prediction, score, file_path, filepath_val_loss,es,skf,train_index,val_index,scores,predicts,epochs,batch_size,\n",
        "\n",
        "# seed는 random으로 지정되며, 결과물이 생성됩니다.\n",
        "def main():\n",
        "  seed = randint(1,1000000)\n",
        "  tdm_train, tdm_test, y_train = data_loading()\n",
        "  creating_results(seed, tdm_train, tdm_test, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYGstb6hWsir"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwu8wE7LiF6h"
      },
      "source": [
        "# py파일을 실행할 때는, 새로운 ipynb를 생성하여\n",
        "- pip install sentencepiece\n",
        "- !python3 'filepath/simple_dnn.py' 를 실행합니다.\n",
        "\n",
        "# 2020년 11월 18일 12시 30분에 생성 결과 : 0.4288\n",
        "\n",
        "#### 다만, 제목과 같이 SIMPLE한 모델입니다. \n",
        "#### 좀 더 tuning하고 regulize한다면 더 좋은 결과를 얻을 수 있을 것이라고 생각합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBLVfeZ0iwDl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}